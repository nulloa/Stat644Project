\documentclass[pdf]{beamer}
\usetheme{Copenhagen}
%\usetheme{AnnArbor}
\usecolortheme{beaver}


\usepackage{graphicx, copyrightbox, bm, amsmath,verbatim}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables


\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Stat 644]{An Intro to Particle MCMC} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Nehemias Ulloa} % Your name
\institute[ISU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Iowa State University \\ % Your institution for the title page
\medskip
\textit{} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

% Title of paper presented
\begin{frame}
Andrieu, C., Doucet, A. and Holenstein, R. (2010), Particle Markov chain Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72: 269â€“342. doi:10.1111/j.1467-9868.2009.00736.x
\end{frame}




\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introduction}
%------------------------------------------------


%------------------------------------------------
\subsection{State Space Models}
%------------------------------------------------


\begin{frame}[fragile]
\theoremstyle{definition}
\begin{definition}
State space models (also called hidden Markov models), $\{ X_n ; n \geq 1 \}$, are be defined by:
\begin{itemize}
  \item Initial density $X_1 \sim \mu_\theta(\cdot)$
  \item Transitional probability density 
  \begin{align}
  X_{n+1}|(X_n = x) \sim f_\theta(\cdot|x) \quad \exists \> \theta \in \Theta
  \end{align}
\end{itemize}
\end{definition}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
A simple example of SSMs are Dynamic Linear Models.

Normal prior at time $t=0$
\begin{align*}
\theta_0 \sim N_p(m_0, C_0)
\end{align*}

and when time $t \geq 1$
\begin{align*}
 Y_t  &= F_t \theta_t + v_t             &   v_t  &\sim N_m(0, V_t) \\
 \theta_t  &= G_t \theta_{t-1} + w_t    &   w_t  &\sim N_p(0, W_t)
\end{align*}
where $F_t, G_t, V_t, W_t$ are known matrices

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
In Bayesian analysis for SSMs, we want to conduct inference on two posteriors:
\begin{enumerate}
  \item $p_\theta(x_{1:T}|y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T})$ assuming $\theta$ is known and where 
  \begin{align}
    p_\theta(x_{1:T}, y_{1:T}) = \mu_\theta(x_1) \prod \limits_{i=2}^T f_\theta(x_n | x_{n-1}) \prod \limits_{n=1}^T g_\theta (y_n | x_n)
  \end{align}
  %
  \item When $\theta$ is unknown,
  \begin{align}
    p(\theta, x_{1:T} | y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T}) p(\theta)
  \end{align}
\end{enumerate}
\end{frame}

%------------------------------------------------
\section{Background}
%------------------------------------------------

%------------------------------------------------
\subsection{Sequential Monte Carlo}
%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
  \item First methods in a Monte Carlo sense to handle SSMs
  \item Big idea: sequentially approximate $p_\theta(x_{1:n}|y_{1:n})$ for $n \geq 1$
  \begin{enumerate}
    \item $p_\theta(x_1 | y_1), p_\theta(y_1)$
    \item $p_\theta(x_{1:2} | y_{1:2}), p_\theta(y_{1:2})$  
    \item $p_\theta(x_{1:n} | y_{1:n}), p_\theta(y_{1:n})$
  \end{enumerate}
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{enumerate}
  \item Generate $k=1,\dots,N$ particles, $\{ X^k_1 \}$, from proposal $q_\theta(x_1 | y_1)$
  \item Calculate weights, $\{ W^k_1 \}$, for each particle
  \item for $n \geq 2$, use weights in sampling $X^k_n$ from $q_\theta(\cdot|y_n, W^k_1)$
\end{enumerate}

At time $T=1,\dots,n$, the approximation is:
\begin{align}
  \hat{p}_\theta(dx_{1:n} | y_{1:n}) := \sum \limits_{k=1}^N W^K_n \delta_{X^k_{1:n}}(dx_{1:n})
\end{align}
where $\delta$ is the Dirac measure
\end{frame}

%------------------------------------------------
\begin{frame}
\includegraphics[width=\textwidth]{SSM.png}
\end{frame}
%------------------------------------------------

\begin{frame}[fragile]
The SMC also provides us with an approximation to the marginal density $p_\theta(y_{1:T})$:
\begin{align}
  \hat{p}_\theta(y_{1:T}) := \hat{p}_\theta(y_1) \prod \limits_{n=2}^T \hat{p}_\theta(y_n|y_{1:n-1})
\end{align}
where
\begin{align*}
  \hat{p}_\theta(y_n|y_{1:n-1}) = \frac{1}{N} \sum \limits_{k=1}^N w_n(X^k_{1:n})
\end{align*}

Problem: $T$ is very large, the number of distinct $x_n$ goes down and difficult to approximate from $p(\theta, x_{1:T}|y_{1:T})$
\end{frame}


%------------------------------------------------
\subsection{MCMC}
%------------------------------------------------


\begin{frame}[fragile]
MCMC methods for state space models focus on sampling from $p(\theta, x_{1:T}|y_{1:T})$ by switching in between sampling from $x_{1:T}|\theta$ and $\theta|x_{1:T}$ \\~\\

$\theta|x_{1:T}, y_{1:T}$ is easy to sample from but $x_{1:T}|y_{1:T}$ rarely is so there is a workaround that splits $x_{1:T}$ into $K$ adjacent blocks and update block by block. \\~\\ 

We would update $x_{n:n+K-1}$ like so:
\begin{align}
  p_\theta(x_{n:n+K-1}|y_{1:T},x_{1:n-1},x_{n+K:T}) \propto \prod \limits_{k=n}^{n+K} f_\theta(x_k|x_{k-1}) \prod \limits_{k=n}^{n+K-1} g_\theta(y_k|x_k) \label{eq:mcmcapprox}
\end{align}

Problem: $K$ becomes too large, then we will have trouble creating good approximations of Eq \ref{eq:mcmcapprox}

\end{frame}




%------------------------------------------------
\section{PMCMC}
%------------------------------------------------

\begin{frame}[fragile]
Let's recall the goal here:
\begin{itemize}
  \item Target $p(\theta, x_{1:T}|y_{1:T})$ 
  \item Best case scenario: sample it via $p(\theta|y_{1:T}) p_\theta(x_{1:T}|y_{1:T})$
  \item Approximate by using the SMC approximation to $p_\theta(x_{1:T}|y_{1:T})$ as a proposal distribution in a Metropolis Hastings update
  \item Not directly feasible
  \item Workaround: include the space of all the random variables generated by SMC in the target distributions considered
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
  \item Particle independent Metropolis-Hastings: $p(x_{1:T} | y_{1:T})$
  \item Particle marginal Metropolis-Hastings: $p(\theta, x_{1:T} | y_{1:T})$
  \item Particle Gibbs: $p(\theta, x_{1:T} | y_{1:T})$
\end{itemize}
\end{frame}



%------------------------------------------------
\subsection{Particle marginal Metropolis-Hastings}
%------------------------------------------------


\begin{frame}[fragile]
\begin{itemize}
\item Interested in $p(\theta, x_{1:T} | y_{1:T})$
\item $p(\theta, x_{1:T} | y_{1:T}) = p(\theta | y_{1:T}) p_\theta(x_{1:T} | y_{1:T})$ 
\item The intuitive proposal is
\begin{equation*}
  q \{ (\theta^*, x^*_{1:T}) | (\theta, x_{1:T}) \} = q(\theta^* | \theta) p_{\theta^*}(x^*_{1:T} | y_{1:T})
\end{equation*}
\item Acceptance ratio:
\begin{equation}
 \frac{p(\theta^*, x^*_{1:T}|y_{1:T})}{p(\theta, x_{1:T}|y_{1:T})}\frac{q \{ (\theta, x_{1:T}) | (\theta^*, x^*_{1:T}) \}}{q \{ (\theta^*, x^*_{1:T}) | (\theta, x_{1:T}) \}} = \frac{p_{\theta^*}(y_{1:T})p(\theta^*)}{p_\theta(y_{1:T})p(\theta)}\frac{q(\theta|\theta^*)}{q(\theta^*|\theta)}
\end{equation}
\end{itemize}

The looking at the acceptance probability, it is natural to use SMC approximation for $p_\theta(x_{1:T}|y_{1:T})$ and $p_\theta(y_{1:T})$ in the PMMH update.
\end{frame}


%------------------------------------------------

\begin{frame}

\begin{block}{Assumption 1}\label{ams:pap2}
For $n=1,\ldots,T$, we have $\mathcal{S}_n \subseteq \mathcal{Q}_n$
where
\begin{align*}
\mathcal{S}_n & = \{ x_{1:T} \in \mathcal(X)^n: p(x_{1:T}) > 0 \} \quad n \geq 1 \\
\mathcal{Q}_n & = \{ x_{1:T} \in \mathcal(X)^n:  p(x_{1:T-1}) q(x_n|x_{1:T-1}) > 0\} \quad n \geq 1
\end{align*}
\end{block}

\begin{block}{Assumption 2}\label{asm:pap2}
For any $k=1,\ldots,N$ and $n=1,\ldots,T$, the resampling scheme satisfies 
\begin{equation}
E(O^k_n|{\bf W}_n) = NW^K_n
\end{equation}
and
\begin{equation}
r(A^K_n = m | {\bf W}_n) = W^m_n
\end{equation}
\end{block}

\end{frame}

%------------------------------------------------

\begin{frame}

\begin{block}{Assumption 3}\label{asm:pap3}
$\exists\text{ a}$ sequence of constants $\{ C_n; n = 1,\ldots,\bar{T} \}$ for some integer $\bar{T}$ s.t. for any $x_{1:n} \in \mathcal{S}_n$
\begin{equation}
w_n(x_{1:n}) \leq C_n
\end{equation}
\end{block}

\begin{block}{Assumption 5}\label{asm:pap5}
For any $\theta \in \mathcal{S}$, we have $\mathcal{S}^\theta_n \subseteq \mathcal{Q}^\theta_n$ for $n=1,\ldots,T$.
\end{block}

\begin{block}{Assumption 6}\label{asm:pap6}
The MH sampler of target density $\pi(\theta)$ and proposal density $q(\theta^*|\theta)$ is irreducible and aperiodic (and hence converges for $\pi$ almost all starting points).
\end{block}

\end{frame}

%------------------------------------------------

\begin{frame}

\begin{alertblock}{Theorem}
Assume assumption 2 holds. Then $\forall N \geq 1$
\begin{enumerate}
  \item the PMMH update is an MH update defined on the extended space $\Theta \times X$ with a target density $\tilde{\pi}$
  %
  \item additionally, if assumptions 5 and 6 are met, the PMMH sampler generates a sequence $\{ \theta(i), X_{1:T}(i) \}$ whose marginal distributions $\{ \mathcal{L}^N \{ \theta(i), X_{1:T}(i) \in \cdot \} \}$ satisfy 
    \begin{equation*}
    \| \mathcal{L}^N \{ \theta(i), X_{1:T}(i) \in \cdot \} - \pi(\cdot) \| \rightarrow 0 \quad \quad \text{as }i \rightarrow \inf
    \end{equation*}
\end{enumerate}
\end{alertblock}

\end{frame}

%------------------------------------------------

\begin{frame}

Also for both PMMH and PG samplers, the author presents a theorem which ensures consistent estimators.
\begin{alertblock}{Theorem}
Assume Assumptions \ref{asm:pap2} - \ref{asm:pap5} and let $f:\Theta \times \mathcal{X}^T \rightarrow \Re$ be such that $E_\pi(|f|) < \infty$. Then when the PMMH or PG sampler is erogodic, the for any $N \geq 1$ or $N \geq 2$ respectively:
\begin{equation*}
  \frac{1}{L} \sum \limits_{i=1}^L \Big[ \sum \limits_{k=1}^N W^k_T(i) f\{ \theta(i), X^k_{1:T}(i) \} \Big] \rightarrow E_\pi(f) \text{ a.s. as }L\rightarrow \infty
\end{equation*}
\end{alertblock}

\end{frame}



%------------------------------------------------
\section{Discussion}
%------------------------------------------------

\begin{frame}
\begin{itemize}
  \item Issue: every iteration in the PMCMC, $N$ particles need to be generated
  \item So basically more computation but better results
  \item PMCMC for SSMs
  \item Been generalized to handle models much more complicated the SSM
\end{itemize}
\end{frame}


\end{document}