\documentclass[pdf]{beamer}
\usetheme{Copenhagen}
%\usetheme{AnnArbor}
\usecolortheme{beaver}


\usepackage{graphicx, copyrightbox, bm, amsmath,verbatim}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables


\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Stat 644]{An Intro to PMCMC} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Nehemias Ulloa} % Your name
\institute[ISU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Iowa State University \\ % Your institution for the title page
\medskip
\textit{} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introduction}
%------------------------------------------------

%------------------------------------------------
\subsection{State Space Models}
%------------------------------------------------


\begin{frame}[fragile]
\theoremstyle{definition}
\begin{definition}
State space models (also called hidden Markov models), $\{ X_n ; n \geq 1 \}$, are be defined by:
\begin{itemize}
  \item Initial density $X_1 \sim \mu_\theta(\cdot)$
  \item Transitional probability density 
  \begin{align}
  X_{n+1}|(X_n = x) \sim f_\theta(\cdot|x) \quad \exists \> \theta \in \Theta
  \end{align}
\end{itemize}
\end{definition}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Talk about Dynamic Linear Models here.
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
In Bayesian analysis, we want to conduct inference on two posteriors:
\begin{enumerate}
  \item $p_\theta(x_{1:T}|y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T})$ assuming $\theta$ is known and where 
  \begin{align}
    p_\theta(x_{1:T}, y_{1:T}) = \mu_\theta(x_1) \prod \limits_{i=2}^T f_\theta(x_n | x_{n-1}) \prod \limits_{n=1}^T g_\theta (y_n | x_n)
  \end{align}
  %
  \item When $\theta$ is unknown,
  \begin{align}
    p(\theta, x_{1:T} | y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T}) p(\theta)
  \end{align}
\end{enumerate}
\end{frame}

%------------------------------------------------
\section{Background}
%------------------------------------------------

%------------------------------------------------
\subsection{Sequential Monte Carlo}
%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
  \item First methods in a Monte Carlo sense it SMC
  \item Big idea: sequentially approximate $p_\theta(x_{1:n}|y_{1:n})$ for $n \geq 1$
  \begin{enumerate}
    \item $p_\theta(x_1 | y_1), p_\theta(y_1)$
    \item $p_\theta(x_{1:2} | y_{1:2}), p_\theta(y_{1:2})$  
    \item $p_\theta(x_{1:n} | y_{1:n}), p_\theta(y_{1:n})$
  \end{enumerate}
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{enumerate}
  \item Generate $N$ particles, $\{ X^k_1 \}$, from proposal $q_\theta(x_1 | y_1)$
  \item Calculate weights, $\{ W^k_1 \}$, for each particle
  \item for $n \geq 2$, use weights in sampling $X^k_n$ from $q_\theta(\cdot|y_n, W^k_1)$
\end{enumerate}

At time $T$, the approximation is:
\begin{align}
  \hat{p}_\theta(dx_{1:n} | y_{1:n}) := \sum \limits_{k=1}^N W^K_n \delta_{X^k_{1:n}}(dx_{1:n})
\end{align}
where $\delta$ is the Dirac measure
\end{frame}






%------------------------------------------------
\section{PMMH}
%------------------------------------------------

%------------------------------------------------

\begin{frame}

\begin{block}{Assumption 1}\label{ams:pap2}
For $n=1,\ldots,T$, we have $\mathcal{S}_n \subseteq \mathcal{Q}_n$
where
\begin{align*}
\mathcal{S}_n & = \{ x_{1:T} \in \mathcal(X)^n: p(x_{1:T}) > 0 \} \quad n \geq 1 \\
\mathcal{Q}_n & = \{ x_{1:T} \in \mathcal(X)^n:  p(x_{1:T-1}) q(x_n|x_{1:T-1}) > 0\} \quad n \geq 1
\end{align*}
\end{block}

\begin{block}{Assumption 2}\label{asm:pap2}
For any $k=1,\ldots,N$ and $n=1,\ldots,T$, the resampling scheme satisfies 
\begin{equation}
E(O^k_n|{\bf W}_n) = NW^K_n
\end{equation}
and
\begin{equation}
r(A^K_n = m | {\bf W}_n) = W^m_n
\end{equation}
\end{block}

\end{frame}

%------------------------------------------------

\begin{frame}

\begin{block}{Assumption 3}\label{asm:pap3}
$\exists\text{ a}$ sequence of constants $\{ C_n; n = 1,\ldots,\bar{T} \}$ for some integer $\bar{T}$ s.t. for any $x_{1:n} \in \mathcal{S}_n$
\begin{equation}
w_n(x_{1:n}) \leq C_n
\end{equation}
\end{block}

\begin{block}{Assumption 5}\label{asm:pap5}
For any $\theta \in \mathcal{S}$, we have $\mathcal{S}^\theta_n \subseteq \mathcal{Q}^\theta_n$ for $n=1,\ldots,T$.
\end{block}

\begin{block}{Assumption 6}\label{asm:pap6}
The MH sampler of target density $\pi(\theta)$ and proposal density $q(\theta^*|\theta)$ is irreducible and aperiodic (and hence converges for $\pi$ almost all starting points).
\end{block}

\end{frame}

%------------------------------------------------

\begin{frame}

\begin{alertblock}{Theorem}
Assume assumption 2 holds. Then $\forall N \geq 1$
\begin{enumerate}
  \item the PMMH update is an MH update defined on the extended space $\Theta \times X$ with a target density $\tilde{\pi}$
  %
  \item additionally, if assumptions 5 and 6 are met, the PMMH sampler generates a sequence $\{ \theta(i), X_{1:T}(i) \}$ whose marginal distributions $\{ \mathcal{L}^N \{ \theta(i), X_{1:T}(i) \in \cdot \} \}$ satisfy 
    \begin{equation*}
    \| \mathcal{L}^N \{ \theta(i), X_{1:T}(i) \in \cdot \} - \pi(\cdot) \| \rightarrow 0 \quad \quad \text{as }i \rightarrow \inf
    \end{equation*}
\end{enumerate}
\end{alertblock}

\end{frame}

%------------------------------------------------

\begin{frame}

Also for both PMMH and PG samplers, the author presents a theorem which ensures consistent estimators.
\begin{alertblock}{Theorem}
Assume Assumptions \ref{asm:pap2} - \ref{asm:pap5} and let $f:\Theta \times \mathcal{X}^T \rightarrow \Re$ be such that $E_\pi(|f|) < \infty$. Then when the PMMH or PG sampler is erogodic, the for any $N \geq 1$ or $N \geq 2$ respectively:
\begin{equation*}
  \frac{1}{L} \sum \limits_{i=1}^L \Big[ \sum \limits_{k=1}^N W^k_T(i) f\{ \theta(i), X^k_{1:T}(i) \} \Big] \rightarrow E_\pi(f) \text{ a.s. as }L\rightarrow \infty
\end{equation*}
\end{alertblock}

\end{frame}



%------------------------------------------------
\section{Discussion}
%------------------------------------------------

\begin{frame}
\begin{itemize}
  \item ALC seems decent for prediction, but NN seems better
  \item It is not clear how much better it is to use ALC vs NN \\~\\
  \item These two examples of GPU usage have contradicting ideas
  \begin{itemize}
    \item One method says that points nearest are most important
    \item One method says that you need to include some points outside of the nearest points
  \end{itemize}
  \item Although it is important to note they set out for different purposes: estimation and prediction
\end{itemize}
\end{frame}


\end{document}