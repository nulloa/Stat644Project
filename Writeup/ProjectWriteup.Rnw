\documentclass{article}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[letterpaper,bindingoffset=0.2in,left=1in,right=1in,top=.8in,bottom=.8in,footskip=.25in]{geometry}

\lhead{Nehemias Ulloa}
\chead{Stat 644}
\rhead{Project}
\lfoot{}
\cfoot{\thepage}
\rfoot{}

% Length to control the \fancyheadoffset and the calculation of \headline
% simultaneously
\newlength\FHoffset
\setlength\FHoffset{1cm}

\addtolength\headwidth{2\FHoffset}

\fancyheadoffset{\FHoffset}

% these lengths will control the headrule trimming to the left and right
\newlength\FHleft
\newlength\FHright

% here the trimmings are controlled by the user
\setlength\FHleft{0cm}
\setlength\FHright{0cm}

\doublespacing

\begin{document}

%-----------------------------
\section{Introduction}
Monte Carlo methods have quickly become one of the most common methods of conducting statistical analysis. This was brought on partly due to the ease of access to powerful computing which previously made these methods unreachable from the layman. However, now that everyone is able to do basic MCMC, the problems researchers bring have become increasingly more difficult to analyze. In this review, we will be looking at the paper, \textit{Particle Markov Chain Monte Carlo Methods}, by Andrieu et al.(2010)\ref{andreiu10} which provides a way to combat the complexity in incoming problems. This paper gives an introduction to Particle Markov Chain Monte Carlo (PMCMC) and gives some intuition behind the algorithm. For this review of the paper, I will focus on introducing the PMCMC; developing the algorithm and intuition behind PMCMC; and provide asymptotic results of the PMCMC algorithm. \\

The paper focuses on a new method called Particle Markov chain Monte Carlo or PMCMC for short. The basic idea behind PMCMC is to combine the best aspects of MCMC methods and Sequential Monte Carlo (SMC) methods. The goal of the algorithm is to use SMC to create proposal densities for the MCMC algorithms. The motivation for PMCMC comes from state space models (SSM), but the ideas and algorithm can be expanded to other high dimensional problems. \\

In general, the goal of a MCMC algorithm is to sample from the target distribution, say $\pi$. Usually, we cannot sample from $\pi$ directly so we choose a proposal distribution which captures the essence of the target distribution and sample from the that. The hardest part of this is the balance between choosing a proposal distribution that adequately describes/captures the target distribution and choosing a proposal distribution that is easy to implement. PMCMC aims to provide a proposal distribution that balances these two features. \\


%-----------------------------
\subsection{State Space Models}

Before we talk about the PMCMC algorithm, we first need to talk about SMC, but before we talk about SMC, we should introduce the motivating example of SMC which are state space models (SSMs). 

{\bf Definition} State space models (also called hidden Markov models), $\{ X_n ; n \geq 1 \}$, are be defined by:
\begin{itemize}
  \item Initial density $X_1 \sim \mu_\theta(\cdot)$
  \item Transitional probability density 
  \begin{align}
  X_{n+1}|(X_n = x) \sim f_\theta(\cdot|x) \exists \theta \in \Theta
  \end{align}
\end{itemize}

Usually, we never see $\{ X_n \}$, but we will ``observe'' them through $\{ Y_n ; n \geq 1 \}$. These $\{ Y_n \}$ are conditionally independent given $\{ X_n \}$. We denote their marginal probability density function by 
\begin{align}
Y_n | (X_1,\ldots,X_n = x,\ldots,X_m) \sim g_\theta(\cdot|x) \text{ for } 1 \leq n \leq m
\end{align}

Notation note: For random variables, capitol letters will be used: $X_n$ and lower case will be used for thier values. Lastly, we will denote a sequence, $\{ z_n \}$, as follows $z_{i:j} = (z_i, z_{i+1},\ldots,z_j)$.

In Bayesian analysis, we want to conduct inference on two posteriors:
\begin{enumerate}
  \item $p_\theta(x_{1:T}|y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T})$ assuming $\theta$ is known and where 
  \begin{align}
    p_\theta(x_{1:T}, y_{1:T}) = \mu_\theta(x_1) \prod \limits_{i=2}^T f_\theta(x_n | x_{n-1}) \prod \limits_{n=1}^T g_\theta (y_n | x_n)
  \end{align}
  %
  \item When $\theta$ is unknown,
  \begin{align}
    p(\theta, x_{1:T} | y_{1:T}) \propto p_\theta(x_{1:T}, y_{1:T}) p(\theta)
  \end{align}
\end{enumerate}

Depending on the type of SSM, these posteriors may be difficult or failry simple to handle. In our case, we will consider the approximations that allow us to handle more complicated cases, and we will outline some background between SMC and MCMC for SSM that help in the development of the approximation.



%-----------------------------
\subsection{Sequential Monte Carlo}

The most common method of handling SSM in a bayesian context involves Sequential Monte Carlo methods (SMC). The big idea of SMC is it provides a method to sequentially approximate $p_\theta(x_{1:n}|y_{1:n})$ for $n \geq 1$ and $p_\theta(y_{1:n})$ using $N$ weighted samples called \textit{particles}; basically we approximate $p_\theta(x_1 | y_1), p_\theta(y_1)$ first, then $p_\theta(x_{1:2} | y_{1:2}), p_\theta(y_{1:2})$ second, all the way to $p_\theta(x_{1:n} | y_{1:n}), p_\theta(y_{1:n})$. The general idea behind the approximation is as follows:
\begin{align}
  \hat{p}_\theta(dx_{1:n} | y_{1:n}) := \sum \limits_{k=1}^N W^K_n \delta_{X^k_{1:n}}(dx_{1:n})
\end{align}
 where $W^k_n$ is the importance weight associated with particle $X^k_{1:n}$. We can see the algrotihm below which outlines the details of the approximation. \\

Here is the algrotihm:
\begin{enumerate}
  \item \textit{Step 1:} at time $n=1$,
  \begin{enumerate}
    \item sample $X^k_1 \sim q_\theta(\cdot|y_1)$
    \item compute \& normalize weights
    \begin{align}
      w_1(X^k_1) :=& \frac{p_\theta(X^k_1, y_1)}{q_\theta(X^k_1 | y_1)} = \frac{\mu_\theta(X^k_1) g_\theta(y_1|X^k_1)}{q_\theta(X^k_1 | y_1)} \nonumber \\
      & W_1^k := \frac{w_1(X_1^k)}{\sum \limits_{m=1}^N w_1(X^m_1)}
    \end{align}
  \end{enumerate}
%
%
  \item \textit{Step 2:} at time $n=2,\ldots,T$,
  \begin{enumerate}
    \item sapmle $A^k_{n-1} \sim \mathcal{F}(\cdot | {\bf W}_{n-1})$
    \item sample $X^k_n \sim q_\theta(\cdot|y_n, X_{n-1}^{A^k_{n-1}})$ and set $X_{1:n}^k := (X_{1:n-1}^{A^k_{n-1}}, X^k_n)$
    \item compute and normalize the weights
    \begin{align}
      w_1(X^k_{1:n}) &:= \frac{p_\theta(X^k_{1:n}, y_{1:n})}{p_\theta(X_{n-1}^{A^k_{n-1}}, y_{1:n-1}) q_\theta(X^k_n|y_n, X_{n-1}^{A^k_{n-1}})} \nonumber \\
                  &= \frac{f_\theta(X^k_n | X_{n-1}^{A^k_{n-1}}) g_\theta(y_n | X^k_n)}{q_\theta(X^k_n | y_n, X_{n-1}^{A^k_{n-1}})} \nonumber \\
      & W_n^k := \frac{w_n(X_{1:n}^k)}{\sum \limits_{m=1}^N w_n(X^m_{1:n})}
    \end{align}
  \end{enumerate}
\end{enumerate}



%-----------------------------
\subsection{Markov chain Monte Carlo}





%------------------------------------%
%           Bibliography             %
%------------------------------------%

\nocite{*}
\bibliographystyle{siam}

\bibliography{writeup_references}

\end{document}